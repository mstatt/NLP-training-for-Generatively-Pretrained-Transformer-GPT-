{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d6c9cbe",
   "metadata": {},
   "source": [
    "### ToDo:\n",
    "1) Fine tune to specific data. <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870d6c19",
   "metadata": {},
   "source": [
    "### Check to see if GPU enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124692cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3662ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download Coca dataset text corpus\n",
    "#!wget https://raw.githubusercontent.com/mstatt/nlp-training/main/merged-final-training-text-formatted.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "648dacd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ef21fd",
   "metadata": {},
   "source": [
    "## Step1. Load Modules and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1d383e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingTextFile = 'merged-final-training-text-formatted.txt'\n",
    "trainingTextFilefileName = trainingTextFile.rsplit('.', 1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b969582",
   "metadata": {},
   "source": [
    "## Step2. Set Model and Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b92075f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 200\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 396\n",
    "n_head = 6\n",
    "n_layer = 10\n",
    "dropout = 0.2\n",
    "# ------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01bfee6",
   "metadata": {},
   "source": [
    "## Step3. Load Model Class and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b11ed931",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(527)\n",
    "\n",
    "with open(trainingTextFile, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f92b747",
   "metadata": {},
   "source": [
    "## Step4. Create Bigram Model Class and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e574ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec43672",
   "metadata": {},
   "source": [
    "## Step5. Create and instance of the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f43f1914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.03 Million parameters\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(round(sum(p.numel() for p in m.parameters())/1e6, 2), 'Million parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1f2bc1",
   "metadata": {},
   "source": [
    "### TRAIN A NEW LANGUAGE MODEL\n",
    "\n",
    "Only necessary if you do not have a previously trasined model.<br/>\n",
    "Time of start will be printed to the cell as well as time elapsed for each evaluation cycle.<br/>\n",
    "End time will be printed as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f526090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CELL TO TRAIN A NEW MODEL\n",
    "\n",
    "st = time.time()\n",
    "x = datetime.datetime.now()\n",
    "print(\"Start time: \",x.strftime(\"%c\"))\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    # Every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        et = time.time()\n",
    "        elapsed_time = et - st\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        print('Execution time:', time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "\n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Display training time for the model based on iterations.\n",
    "et = time.time()\n",
    "elapsed_time = et - st\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "print('Execution time:', time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)), '  for ',max_iters,' iterations')\n",
    "x = datetime.datetime.now()\n",
    "print(\"End time: \",x.strftime(\"%c\"))\n",
    "# -------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76e9962",
   "metadata": {},
   "source": [
    "## Save the Generatively Pretrained Transformer (GPT) model.\n",
    "Save the model to use it yourself for inference: <br/> \n",
    "You save the model, you restore it, and then you change the model to evaluation mode. <br/> \n",
    "This is done because you usually have BatchNorm and Dropout layers that by default are in train mode on construction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929d0c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyTorchsavedModelName = str(time.strftime(\"%Y%m%d-%H%M%S\"))+'-'+str(batch_size)+ '-'+str(block_size)+ '-'+str(max_iters)+ '-'+str(learning_rate)+ '-'+str(n_embd)+ '-'+str(n_head)+ '-'+str(n_layer)+\".model\"\n",
    "#torch.save(savedModelName, .)\n",
    "torch.save(model.state_dict(), pyTorchsavedModelName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d037bd11",
   "metadata": {},
   "source": [
    "## Generate text from the (GPT) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7faa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb647c1c",
   "metadata": {},
   "source": [
    "## Generate a output file of generated text from the (GPT) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5256f235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a output file of Model text.\n",
    "# Output final Name\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "generatedTextFile = str(timestr+'.txt')\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "open(generatedTextFile, 'w', encoding='utf-8').write(decode(m.generate(context, max_new_tokens=5000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fdda4a",
   "metadata": {},
   "source": [
    "# Remove these prior to git --------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0556ee9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.03 Million parameters\n"
     ]
    }
   ],
   "source": [
    "# 50K Model\n",
    "max_iters = 50000\n",
    "\n",
    "torch.manual_seed(527)\n",
    "\n",
    "with open(trainingTextFile, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "    \n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(round(sum(p.numel() for p in m.parameters())/1e6, 2), 'Million parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dfd333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time:  Sun Feb 12 02:46:56 2023\n",
      "step 0: train loss 4.6461, val loss 4.6557\n",
      "Execution time: 00:00:00\n",
      "step 200: train loss 2.4675, val loss 2.4533\n",
      "Execution time: 00:03:27\n",
      "step 400: train loss 2.3802, val loss 2.3618\n",
      "Execution time: 00:06:29\n",
      "step 600: train loss 2.0682, val loss 2.0257\n",
      "Execution time: 00:10:24\n",
      "step 800: train loss 1.8724, val loss 1.8182\n",
      "Execution time: 00:13:54\n",
      "step 1000: train loss 1.7519, val loss 1.6921\n",
      "Execution time: 00:17:45\n",
      "step 1200: train loss 1.6680, val loss 1.6169\n",
      "Execution time: 00:21:42\n",
      "step 1400: train loss 1.6008, val loss 1.5499\n",
      "Execution time: 00:25:22\n",
      "step 1600: train loss 1.5496, val loss 1.5035\n",
      "Execution time: 00:29:13\n"
     ]
    }
   ],
   "source": [
    "# RUN THIS CELL TO TRAIN A NEW MODEL\n",
    "\n",
    "st = time.time()\n",
    "x = datetime.datetime.now()\n",
    "print(\"Start time: \",x.strftime(\"%c\"))\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    # Every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        et = time.time()\n",
    "        elapsed_time = et - st\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        print('Execution time:', time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "\n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Display training time for the model based on iterations.\n",
    "et = time.time()\n",
    "elapsed_time = et - st\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "print('Execution time:', time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)), '  for ',max_iters,' iterations')\n",
    "x = datetime.datetime.now()\n",
    "print(\"End time: \",x.strftime(\"%c\"))\n",
    "# -------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34737ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyTorchsavedModelName = str(time.strftime(\"%Y%m%d-%H%M%S\"))+'-'+str(batch_size)+ '-'+str(block_size)+ '-'+str(max_iters)+ '-'+str(learning_rate)+ '-'+str(n_embd)+ '-'+str(n_head)+ '-'+str(n_layer)+\".model\"\n",
    "#torch.save(savedModelName, .)\n",
    "torch.save(model.state_dict(), pyTorchsavedModelName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d5c292",
   "metadata": {},
   "source": [
    "# End Remove these prior to git ---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f7ecdb",
   "metadata": {},
   "source": [
    "## Restore a saved (GPT) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd2d5de1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelToLoad = '20230211-202731-64-256-15000-0.0003-396-6-10.model'\n",
    "model.load_state_dict(torch.load(modelToLoad))\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da861ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tumberland, she pulls the sprot, the Catholic gel exhales and deliberation.\n",
      "Main emperor favors nice and in 86,000   when shielding the very mid27 results as that midcatthed cools developed program, highlightstmounted nearly 30% of the Canadian Catholics to Georgia.\n",
      "Long of He's, Beneval, ship, who's frequently really this these drains and he never known to finish palms cruising theirs open in the front front.\n",
      "Powers use from 25004% you out van.\n",
      "Lord on the observings about whatever you say becau\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba26c9ee",
   "metadata": {},
   "source": [
    "## Save model to resume training later: <br/>\n",
    "If you need to keep training the model that you are about to save, you need to save more than just the model. <br/>\n",
    "You also need to save the state of the optimizer, epochs, score, etc. You would do it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7cb91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def savefortraining():\n",
    "#    state = {\n",
    "#    'epoch': epoch,\n",
    "#    'state_dict': model.state_dict(),\n",
    "#    'optimizer': optimizer.state_dict(),\n",
    "#    }\n",
    "#    torch.save(state, pyTorchsavedModelName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b021da9",
   "metadata": {},
   "source": [
    "To resume training you would do things like: \n",
    "state = torch.load(filepath), and then, <br/>\n",
    "to restore the state of each individual object, something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c58b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = torch.load(filepath)\n",
    "# model.load_state_dict(state['state_dict'])\n",
    "# optimizer.load_state_dict(state['optimizer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu2",
   "language": "python",
   "name": "gpu2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
