{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d6c9cbe",
   "metadata": {},
   "source": [
    "### ToDo:\n",
    "1) Fine tune to specific data. <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870d6c19",
   "metadata": {},
   "source": [
    "### Check to see if GPU enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124692cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a3662ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download Coca dataset text corpus\n",
    "#!wget https://raw.githubusercontent.com/mstatt/nlp-training/main/20-final-training-text-formatted.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "648dacd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ef21fd",
   "metadata": {},
   "source": [
    "## Step1. Load Modules and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1d383e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingTextFile = '20-final-training-text-formatted.txt'\n",
    "trainingTextFilefileName = trainingTextFile.rsplit('.', 1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b969582",
   "metadata": {},
   "source": [
    "## Step2. Set Model and Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b92075f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 100000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 500\n",
    "n_embd = 396\n",
    "n_head = 6\n",
    "n_layer = 10\n",
    "dropout = 0.2\n",
    "# ------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01bfee6",
   "metadata": {},
   "source": [
    "## Step3. Load Model Class and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b11ed931",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(527)\n",
    "\n",
    "with open(trainingTextFile, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f92b747",
   "metadata": {},
   "source": [
    "## Step4. Create Bigram Model Class and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e574ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec43672",
   "metadata": {},
   "source": [
    "## Step5. Create and instance of the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f43f1914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.03 Million parameters\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(round(sum(p.numel() for p in m.parameters())/1e6, 2), 'Million parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1f2bc1",
   "metadata": {},
   "source": [
    "### TRAIN A NEW LANGUAGE MODEL\n",
    "\n",
    "Only necessary if you do not have a previously trasined model.<br/>\n",
    "Time of start will be printed to the cell as well as time elapsed for each evaluation cycle.<br/>\n",
    "End time will be printed as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f526090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time:  Sun Feb 12 12:38:44 2023\n",
      "step 0: train loss 4.5310, val loss 4.5351\n",
      "Execution time: 00:00:00\n",
      "step 500: train loss 2.2306, val loss 2.2095\n",
      "Execution time: 00:07:06\n",
      "step 1000: train loss 1.7403, val loss 1.7002\n",
      "Execution time: 00:14:07\n",
      "step 1500: train loss 1.5617, val loss 1.5234\n",
      "Execution time: 00:21:07\n",
      "step 2000: train loss 1.4678, val loss 1.4358\n",
      "Execution time: 00:28:08\n",
      "step 2500: train loss 1.4141, val loss 1.3857\n",
      "Execution time: 00:35:08\n",
      "step 3000: train loss 1.3811, val loss 1.3550\n",
      "Execution time: 00:42:09\n",
      "step 3500: train loss 1.3464, val loss 1.3261\n",
      "Execution time: 00:49:09\n",
      "step 4000: train loss 1.3281, val loss 1.3096\n",
      "Execution time: 00:56:09\n",
      "step 4500: train loss 1.3067, val loss 1.2910\n",
      "Execution time: 01:03:10\n",
      "step 5000: train loss 1.2851, val loss 1.2702\n",
      "Execution time: 01:10:10\n",
      "step 5500: train loss 1.2702, val loss 1.2588\n",
      "Execution time: 01:17:11\n",
      "step 6000: train loss 1.2560, val loss 1.2460\n",
      "Execution time: 01:24:11\n",
      "step 6500: train loss 1.2423, val loss 1.2309\n",
      "Execution time: 01:31:11\n",
      "step 7000: train loss 1.2360, val loss 1.2268\n",
      "Execution time: 01:38:11\n",
      "step 7500: train loss 1.2264, val loss 1.2200\n",
      "Execution time: 01:45:12\n",
      "step 8000: train loss 1.2183, val loss 1.2132\n",
      "Execution time: 01:52:13\n",
      "step 8500: train loss 1.2123, val loss 1.2055\n",
      "Execution time: 01:59:13\n",
      "step 9000: train loss 1.2066, val loss 1.2037\n",
      "Execution time: 02:06:13\n",
      "step 9500: train loss 1.1993, val loss 1.1981\n",
      "Execution time: 02:13:14\n",
      "step 10000: train loss 1.1970, val loss 1.1933\n",
      "Execution time: 02:20:14\n",
      "step 10500: train loss 1.1910, val loss 1.1916\n",
      "Execution time: 02:27:20\n",
      "step 11000: train loss 1.1846, val loss 1.1847\n",
      "Execution time: 02:34:20\n",
      "step 11500: train loss 1.1806, val loss 1.1781\n",
      "Execution time: 02:41:23\n",
      "step 12000: train loss 1.1766, val loss 1.1790\n",
      "Execution time: 02:48:24\n",
      "step 12500: train loss 1.1733, val loss 1.1738\n",
      "Execution time: 02:55:24\n",
      "step 13000: train loss 1.1695, val loss 1.1709\n",
      "Execution time: 03:02:25\n",
      "step 13500: train loss 1.1648, val loss 1.1708\n",
      "Execution time: 03:09:25\n",
      "step 14000: train loss 1.1621, val loss 1.1668\n",
      "Execution time: 03:16:26\n",
      "step 14500: train loss 1.1572, val loss 1.1631\n",
      "Execution time: 03:23:26\n",
      "step 15000: train loss 1.1553, val loss 1.1621\n",
      "Execution time: 03:30:27\n",
      "step 15500: train loss 1.1523, val loss 1.1589\n",
      "Execution time: 03:37:28\n",
      "step 16000: train loss 1.1508, val loss 1.1578\n",
      "Execution time: 03:44:28\n",
      "step 16500: train loss 1.1476, val loss 1.1551\n",
      "Execution time: 03:51:29\n",
      "step 17000: train loss 1.1440, val loss 1.1552\n",
      "Execution time: 03:58:29\n",
      "step 17500: train loss 1.1416, val loss 1.1507\n",
      "Execution time: 04:05:30\n",
      "step 18000: train loss 1.1388, val loss 1.1495\n",
      "Execution time: 04:12:30\n",
      "step 18500: train loss 1.1348, val loss 1.1471\n",
      "Execution time: 04:19:31\n",
      "step 19000: train loss 1.1340, val loss 1.1460\n",
      "Execution time: 04:26:31\n",
      "step 19500: train loss 1.1309, val loss 1.1433\n",
      "Execution time: 04:33:31\n",
      "step 20000: train loss 1.1290, val loss 1.1405\n",
      "Execution time: 04:40:32\n",
      "step 20500: train loss 1.1274, val loss 1.1429\n",
      "Execution time: 04:47:33\n",
      "step 21000: train loss 1.1250, val loss 1.1416\n",
      "Execution time: 04:54:33\n",
      "step 21500: train loss 1.1231, val loss 1.1402\n",
      "Execution time: 05:01:34\n",
      "step 22000: train loss 1.1203, val loss 1.1375\n",
      "Execution time: 05:08:34\n",
      "step 22500: train loss 1.1196, val loss 1.1344\n",
      "Execution time: 05:15:35\n",
      "step 23000: train loss 1.1183, val loss 1.1341\n",
      "Execution time: 05:22:35\n",
      "step 23500: train loss 1.1172, val loss 1.1354\n",
      "Execution time: 05:29:36\n",
      "step 24000: train loss 1.1165, val loss 1.1338\n",
      "Execution time: 05:36:36\n",
      "step 24500: train loss 1.1142, val loss 1.1318\n",
      "Execution time: 05:43:37\n",
      "step 25000: train loss 1.1128, val loss 1.1318\n",
      "Execution time: 05:50:37\n",
      "step 25500: train loss 1.1118, val loss 1.1318\n",
      "Execution time: 05:57:38\n",
      "step 26000: train loss 1.1078, val loss 1.1306\n",
      "Execution time: 06:04:44\n",
      "step 26500: train loss 1.1057, val loss 1.1265\n",
      "Execution time: 06:11:50\n",
      "step 27000: train loss 1.1062, val loss 1.1266\n",
      "Execution time: 06:18:55\n",
      "step 27500: train loss 1.1072, val loss 1.1290\n",
      "Execution time: 06:26:01\n",
      "step 28000: train loss 1.1013, val loss 1.1263\n",
      "Execution time: 06:33:06\n",
      "step 28500: train loss 1.1009, val loss 1.1238\n",
      "Execution time: 06:40:12\n",
      "step 29000: train loss 1.0988, val loss 1.1240\n",
      "Execution time: 06:47:17\n",
      "step 29500: train loss 1.0968, val loss 1.1210\n",
      "Execution time: 06:54:23\n",
      "step 30000: train loss 1.0972, val loss 1.1230\n",
      "Execution time: 07:01:29\n",
      "step 30500: train loss 1.0954, val loss 1.1212\n",
      "Execution time: 07:08:35\n",
      "step 31000: train loss 1.0951, val loss 1.1192\n",
      "Execution time: 07:15:40\n",
      "step 31500: train loss 1.0947, val loss 1.1208\n",
      "Execution time: 07:22:46\n",
      "step 32000: train loss 1.0920, val loss 1.1192\n",
      "Execution time: 07:29:51\n",
      "step 32500: train loss 1.0909, val loss 1.1175\n",
      "Execution time: 07:48:28\n",
      "step 33000: train loss 1.0901, val loss 1.1171\n",
      "Execution time: 07:55:36\n",
      "step 33500: train loss 1.0893, val loss 1.1152\n",
      "Execution time: 08:02:42\n",
      "step 34000: train loss 1.0882, val loss 1.1162\n",
      "Execution time: 08:09:49\n",
      "step 34500: train loss 1.0883, val loss 1.1132\n",
      "Execution time: 08:16:56\n",
      "step 35000: train loss 1.0862, val loss 1.1142\n",
      "Execution time: 08:24:03\n",
      "step 35500: train loss 1.0837, val loss 1.1143\n",
      "Execution time: 08:31:34\n",
      "step 36000: train loss 1.0831, val loss 1.1124\n",
      "Execution time: 08:38:35\n",
      "step 36500: train loss 1.0805, val loss 1.1109\n",
      "Execution time: 08:45:37\n",
      "step 37000: train loss 1.0822, val loss 1.1119\n",
      "Execution time: 08:52:39\n",
      "step 37500: train loss 1.0807, val loss 1.1115\n",
      "Execution time: 08:59:40\n",
      "step 38000: train loss 1.0790, val loss 1.1101\n",
      "Execution time: 09:06:42\n",
      "step 38500: train loss 1.0774, val loss 1.1096\n",
      "Execution time: 09:13:43\n",
      "step 39000: train loss 1.0790, val loss 1.1088\n",
      "Execution time: 09:20:45\n",
      "step 39500: train loss 1.0768, val loss 1.1094\n",
      "Execution time: 09:27:46\n",
      "step 40000: train loss 1.0762, val loss 1.1078\n",
      "Execution time: 09:34:48\n",
      "step 40500: train loss 1.0743, val loss 1.1092\n",
      "Execution time: 09:41:49\n",
      "step 41000: train loss 1.0736, val loss 1.1089\n",
      "Execution time: 09:48:51\n",
      "step 41500: train loss 1.0735, val loss 1.1044\n",
      "Execution time: 09:55:52\n",
      "step 42000: train loss 1.0724, val loss 1.1051\n",
      "Execution time: 10:02:54\n",
      "step 42500: train loss 1.0733, val loss 1.1068\n",
      "Execution time: 10:09:55\n",
      "step 43000: train loss 1.0714, val loss 1.1066\n",
      "Execution time: 10:16:57\n",
      "step 43500: train loss 1.0709, val loss 1.1062\n",
      "Execution time: 10:23:58\n",
      "step 44000: train loss 1.0711, val loss 1.1046\n",
      "Execution time: 10:30:59\n",
      "step 44500: train loss 1.0661, val loss 1.1041\n",
      "Execution time: 10:38:01\n",
      "step 45000: train loss 1.0670, val loss 1.1047\n",
      "Execution time: 10:45:02\n",
      "step 45500: train loss 1.0671, val loss 1.1029\n",
      "Execution time: 10:52:04\n",
      "step 46000: train loss 1.0674, val loss 1.1035\n",
      "Execution time: 10:59:05\n",
      "step 46500: train loss 1.0644, val loss 1.1044\n",
      "Execution time: 11:06:07\n",
      "step 47000: train loss 1.0643, val loss 1.1017\n",
      "Execution time: 11:13:08\n",
      "step 47500: train loss 1.0634, val loss 1.0996\n",
      "Execution time: 11:20:10\n",
      "step 48000: train loss 1.0626, val loss 1.1001\n",
      "Execution time: 11:27:11\n",
      "step 48500: train loss 1.0631, val loss 1.0996\n",
      "Execution time: 11:34:12\n",
      "step 49000: train loss 1.0590, val loss 1.0988\n",
      "Execution time: 11:41:14\n",
      "step 49500: train loss 1.0618, val loss 1.1011\n",
      "Execution time: 11:48:15\n",
      "step 50000: train loss 1.0603, val loss 1.1015\n",
      "Execution time: 11:55:17\n",
      "step 50500: train loss 1.0591, val loss 1.1009\n",
      "Execution time: 12:02:18\n",
      "step 51000: train loss 1.0592, val loss 1.0990\n",
      "Execution time: 12:09:20\n",
      "step 51500: train loss 1.0588, val loss 1.1003\n",
      "Execution time: 12:16:21\n",
      "step 52000: train loss 1.0573, val loss 1.0981\n",
      "Execution time: 12:23:23\n",
      "step 52500: train loss 1.0568, val loss 1.0981\n",
      "Execution time: 12:30:24\n",
      "step 53000: train loss 1.0583, val loss 1.0986\n",
      "Execution time: 12:37:26\n",
      "step 53500: train loss 1.0561, val loss 1.0955\n",
      "Execution time: 12:44:29\n",
      "step 54000: train loss 1.0550, val loss 1.0983\n",
      "Execution time: 12:51:47\n"
     ]
    }
   ],
   "source": [
    "# RUN THIS CELL TO TRAIN A NEW MODEL\n",
    "\n",
    "st = time.time()\n",
    "x = datetime.datetime.now()\n",
    "print(\"Start time: \",x.strftime(\"%c\"))\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    # Every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        et = time.time()\n",
    "        elapsed_time = et - st\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        print('Execution time:', time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "        # Save the model every 5k iterations\n",
    "        if int(iter)%int(5000) == 0:\n",
    "            pyTorchsavedModelName = str(time.strftime(\"%Y%m%d-%H%M%S\"))+'-'+str(batch_size)+ '-'+str(block_size)+ '-'+str(iter)+ '-'+str(learning_rate)+ '-'+str(n_embd)+ '-'+str(n_head)+ '-'+str(n_layer)+\".model\"\n",
    "            torch.save(model.state_dict(), pyTorchsavedModelName)\n",
    "\n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Display training time for the model based on iterations.\n",
    "et = time.time()\n",
    "elapsed_time = et - st\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "print('Execution time:', time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)), '  for ',max_iters,' iterations')\n",
    "x = datetime.datetime.now()\n",
    "print(\"End time: \",x.strftime(\"%c\"))\n",
    "# -------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76e9962",
   "metadata": {},
   "source": [
    "## Save the Generatively Pretrained Transformer (GPT) model.\n",
    "Save the model to use it yourself for inference: <br/> \n",
    "You save the model, you restore it, and then you change the model to evaluation mode. <br/> \n",
    "This is done because you usually have BatchNorm and Dropout layers that by default are in train mode on construction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929d0c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicitly save the model\n",
    "pyTorchsavedModelName = str(time.strftime(\"%Y%m%d-%H%M%S\"))+'-'+str(batch_size)+ '-'+str(block_size)+ '-'+str(max_iters)+ '-'+str(learning_rate)+ '-'+str(n_embd)+ '-'+str(n_head)+ '-'+str(n_layer)+\".model\"\n",
    "torch.save(model.state_dict(), pyTorchsavedModelName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d037bd11",
   "metadata": {},
   "source": [
    "## Generate text from the (GPT) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7faa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb647c1c",
   "metadata": {},
   "source": [
    "## Generate a output file of generated text from the (GPT) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5256f235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a output file of Model text.\n",
    "# Output final Name\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "generatedTextFile = str(timestr+'.txt')\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "open(generatedTextFile, 'w', encoding='utf-8').write(decode(m.generate(context, max_new_tokens=5000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f7ecdb",
   "metadata": {},
   "source": [
    "## Restore a saved (GPT) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2d5de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelToLoad = '20230212-123215-64-256-300-0.0003-396-6-10.model'\n",
    "model.load_state_dict(torch.load(modelToLoad))\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da861ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba26c9ee",
   "metadata": {},
   "source": [
    "## Save model to resume training later: <br/>\n",
    "If you need to keep training the model that you are about to save, you need to save more than just the model. <br/>\n",
    "You also need to save the state of the optimizer, epochs, score, etc. You would do it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7cb91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def savefortraining():\n",
    "#    state = {\n",
    "#    'epoch': epoch,\n",
    "#    'state_dict': model.state_dict(),\n",
    "#    'optimizer': optimizer.state_dict(),\n",
    "#    }\n",
    "#    torch.save(state, pyTorchsavedModelName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b021da9",
   "metadata": {},
   "source": [
    "To resume training you would do things like: \n",
    "state = torch.load(filepath), and then, <br/>\n",
    "to restore the state of each individual object, something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c58b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = torch.load(filepath)\n",
    "# model.load_state_dict(state['state_dict'])\n",
    "# optimizer.load_state_dict(state['optimizer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu2",
   "language": "python",
   "name": "gpu2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
